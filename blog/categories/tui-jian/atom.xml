<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 推荐 | 数据矿工]]></title>
  <link href="http://haoyuan-hu.github.io/blog/categories/tui-jian/atom.xml" rel="self"/>
  <link href="http://haoyuan-hu.github.io/"/>
  <updated>2013-12-18T16:02:31+08:00</updated>
  <id>http://haoyuan-hu.github.io/</id>
  <author>
    <name><![CDATA[胡浩源]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[实时策略更换]]></title>
    <link href="http://haoyuan-hu.github.io/blog/2013/12/18/online-turn/"/>
    <updated>2013-12-18T14:53:55+08:00</updated>
    <id>http://haoyuan-hu.github.io/blog/2013/12/18/online-turn</id>
    <content type="html"><![CDATA[<p>传统的推荐:</p>

<ul>
<li>离线训练</li>
<li>离线预测</li>
<li>利用kv存储和获取结果</li>
</ul>


<p>而对于实时数据的引入， 一部分的做法是把实时的在线数据采集下来， 在离线做训练， 获得权重以后， 做一个在线预测的过程：</p>

<ol>
<li>收集实时数据， 做离线训练</li>
<li>做在线预测</li>
</ol>


<p>另一部分采用的是online learning, 利用用户不停的行为反馈（包含显式的和隐式的）， 来调整特征权重。</p>

<p>现在在考虑一种在线增强学习的方式， 假定我们现在有四种投放策略：a, b, c, d。
对于缺乏历史信息的新用户， 在没有任何信息的情况下， 我们先以轮播的方式在所有的位置投放a, b, c, d四种策略。
随后我们采集相应的反馈信息：</p>

<ul>
<li>点击：显式正反馈</li>
<li>删除： 显式负反馈</li>
<li>未点击： 隐式负反馈</li>
</ul>


<p>在这一过程中快速积累用户对不同策略的敏感程度，同时对离线的预测结果进行纠正。</p>

<p>由于单独位置对用户的曝光机会有限， 需要以打通的眼光看待所有的投放位置， 将他作为一个整体：</p>

<ol>
<li>收集到显式负反馈的策略在全局降分</li>
<li>收集到显式负反馈的目标在全局过滤</li>
</ol>


<p>最轻量级的快速尝试方式是， 对于某部分用户而言， 预估的投放策略可信度已经低于热门了， 或者补全的时候采用相似算法的可信度已经低于热门了， 就可以通过快速切换策略达到在线选择的目的。</p>
]]></content>
  </entry>
  
</feed>
