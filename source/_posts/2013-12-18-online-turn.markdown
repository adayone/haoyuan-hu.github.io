---
layout: post
title: "实时策略更换"
date: 2013-12-18 14:53:55 +0800
comments: true
categories: 推荐
---
传统的推荐:

* 离线训练
* 离线预测
* 利用kv存储和获取结果

而对于实时数据的引入， 一部分采用的是：

1. 收集实时数据， 做离线训练
2. 做在线预测

另一部分采用的是online learning, 利用用户不停的行为反馈（包含显式的和隐式的）， 来调整特征权重。

现在在考虑一种在线增强学习的方式， 假定我们现在有四种投放策略：a, b, c, d。
对于缺乏历史信息的新用户， 在没有任何信息的情况下， 我们先以轮播的方式在所有的位置投放a, b, c, d四种策略。
随后我们采集相应的反馈信息：

* 点击：显式正反馈
* 删除： 显式负反馈
* 未点击： 隐式负反馈

在这一过程中快速积累用户对不同策略的敏感程度，同时对离线的预测结果进行纠正。

由于单独位置对用户的曝光机会有限， 需要以打通的眼光看待所有的投放位置， 将他作为一个整体：

1. 收集到显式负反馈的策略在全局降分
2. 收集到显式负反馈的目标在全局过滤

最轻量级的快速尝试方式是， 对于某部分用户而言， 预估的投放策略可信度已经低于热门了， 或者补全的时候采用相似算法的可信度已经低于热门了， 就可以通过快速切换策略达到在线选择的目的。
